{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdogAJjZNjt6"
      },
      "source": [
        "# KoGPT 모델 Prompt-Tuning\n",
        "\n",
        "- 학습 내용\n",
        "  - [KoGPT](https://github.com/kakaobrain/kogpt) 파라미터를 직접 Fine-Tuning하는 대신 Prompt-Tuning을 사용합니다.\n",
        "  - HuggingFace에서 제공하는 [PEFT: Parameter-Efficient Fine-Tuning](https://github.com/huggingface/peft) 라이브러리를 활용합니다.\n",
        "  - 학습 결과는 1MB 정도 크기의 프롬프트 정보만 저장해도 재사용 할 수 있습니다.\n",
        "\n",
        "  \n",
        "- 환경 필요사항\n",
        "  - 학습에는 GPU 메모리가 많이 필요하므로 colab 사용 시 유료 요금제 사용을 권장합니다.\n",
        "  - 구글 드라이브에 `/GPT_Competition/train.csv`, `/GPT_Competition/test.csv`가 업로드 되어있어야합니다.\n",
        "  - 학습 결과를 구글드라이브에 저장하므로 약간의 여유공간이 필요합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9BEIry22gvj"
      },
      "outputs": [],
      "source": [
        "# 구글 드라이브와 연동합니다. 권한 허용이 필요합니다.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3PKbDF12aGw"
      },
      "outputs": [],
      "source": [
        "# 라이브러리를 설치합니다.\n",
        "%pip install -q transformers datasets accelerate\n",
        "%pip install -q peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M61qfK-72nBf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, gc\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "\n",
        "import datasets\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
        "\n",
        "# HuggingFace peft 라이브러리\n",
        "from peft import get_peft_model, PeftModel, TaskType, LoraConfig, PromptTuningConfig, PromptTuningInit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo6QTgK3RbMk"
      },
      "source": [
        "## 데이터 셋 정의하기\n",
        "- HuggingFace의 Dataset 클래스를 활용합니다. batch 단위로 전처리하기 편리합니다.\n",
        "- tokenizer 클래스는 batch 단위로 처리하면 속도가 빠릅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phFCL-hl2qB7"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z8rJ9_F2r3w"
      },
      "outputs": [],
      "source": [
        "# 테스트 데이터를 HuggingFace Dataset으로 불러옵니다.\n",
        "data_path = '/content/drive/MyDrive/GPT_Competition/train.csv'\n",
        "train_df = pd.read_csv(data_path)\n",
        "train_set = datasets.Dataset.from_pandas(train_df)\n",
        "del train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8pexeK4USV0"
      },
      "source": [
        "- [Batch mapping](https://huggingface.co/docs/datasets/about_map_batch) 기능을 활용하여 데이터 셋 전체를 미리 토큰화합니다.\n",
        "- `{text} 한줄 요약: {summary} [EOS]` 형태로 input을 만듭니다. 문장 끝 토큰 `[EOS]`를 직접 추가합니다.\n",
        "- input에서 생성하려는 `{summary} [EOS]` 부분만 정답 label로 남기고 나머지는 ignore_index인 -100으로 가립니다.\n",
        "- [GPTJForCausalLM](https://huggingface.co/docs/transformers/model_doc/gptj#transformers.GPTJForCausalLM) 모델에 input_ids와 labels를 넘겨주면 logits과 loss를 계산해줍니다.\n",
        "- loss 는 모델이 예측한 logits과 정답 label의 [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)로 계산하며 -100인 부분은 계산에서 제외됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syGeAV_eUbf3"
      },
      "outputs": [],
      "source": [
        "def train_batch_preprocess(batch):\n",
        "    prompt = \"{text} 한줄 요약:\"\n",
        "    query_text = [prompt.format(text=text) for text in batch['text']]\n",
        "    target_text = batch['summary']\n",
        "    query = tokenizer(query_text)\n",
        "    target = tokenizer(target_text)\n",
        "\n",
        "    input_ids = [q + t + [tokenizer.eos_token_id] for q, t in zip(query['input_ids'], target['input_ids'])]\n",
        "    attention_mask = [q + t + [1] for q, t in zip(query['attention_mask'], target['attention_mask'])]\n",
        "    labels = [[-100] * len(q) + t + [tokenizer.eos_token_id] for q, t in zip(query['input_ids'], target['input_ids'])]\n",
        "\n",
        "    # 결과로 돌려주는 값들이 추가됩니다.\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67NvxO6M3Drw"
      },
      "outputs": [],
      "source": [
        "# batch단위로 전처리를 실행합니다.\n",
        "# 토큰화 이후에 id, text, summary는 필요없으므로 버립니다.\n",
        "train_set = train_set.map(\n",
        "    train_batch_preprocess,\n",
        "    remove_columns = ['id', 'text', 'summary'],\n",
        "    batched = True,\n",
        "    batch_size = 1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0ElL1JuZXWn"
      },
      "outputs": [],
      "source": [
        "# 결과를 확인합니다.\n",
        "# (eos_token_id = 1, ignore_index = -100)\n",
        "print(train_set)\n",
        "print(len(train_set[0]['input_ids']))\n",
        "print(train_set[0]['input_ids'])\n",
        "print(train_set[0]['attention_mask'])\n",
        "print(train_set[0]['labels'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VSZ9VwKbI-1"
      },
      "source": [
        "- train_set은 각 샘플마다 길이가 다릅니다.\n",
        "- 길이가 서로 다른 샘플을 하나의 배치로 만들기 위해 collate함수를 작성합니다.\n",
        "- GPTJ는 문장의 오른쪽 끝부터 생성하는 autoregressive 모델이므로 오른쪽 끝이 같아야합니다. left padding을 사용합니다.\n",
        "- attention_mask는 0, labels는 -100으로 padding합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFFO3wM0CU30"
      },
      "outputs": [],
      "source": [
        "def left_pad(sequence, value, max_len):\n",
        "    return [value] * (max_len - len(sequence)) + sequence\n",
        "\n",
        "def collate_fn(batch, device='cuda'):\n",
        "    length = max(len(row['input_ids']) for row in batch)\n",
        "    input_ids = [\n",
        "        left_pad(row['input_ids'], tokenizer.pad_token_id, length)\n",
        "        for row in batch\n",
        "    ]\n",
        "    attention_mask = [\n",
        "        left_pad(row['attention_mask'], 0, length)\n",
        "        for row in batch\n",
        "    ]\n",
        "    labels = [\n",
        "        left_pad(row['input_ids'], -100, length)\n",
        "        for row in batch\n",
        "    ]\n",
        "    return {\n",
        "        'input_ids': torch.tensor(input_ids, dtype=torch.long , device=device),\n",
        "        'attention_mask': torch.tensor(attention_mask, dtype=torch.long , device=device),\n",
        "        'labels': torch.tensor(labels, dtype=torch.long , device=device),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5XKyXZvCDyx"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_set, batch_size=2, shuffle=True, num_workers=0,\n",
        "    collate_fn=collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KC4ahfScaDr"
      },
      "source": [
        "## 모델 불러오기\n",
        "- base model을 불러온뒤 peft model로 감싸줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKAeznLYApWA"
      },
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    'kakaobrain/kogpt', revision = 'KoGPT6B-ryan1.5b-float16',\n",
        "    torch_dtype = torch.float16,\n",
        "    device_map = 'auto',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHc92vhRdmel"
      },
      "source": [
        "config\n",
        "- TaskType: 앞문장에서 뒷문장을 생성하는 CAUSAL_LM입니다.\n",
        "- virtual_token_num: soft prompt에 사용할 토큰 수입니다. input 앞부분에 추가됩니다.\n",
        "- prompt_tuning_init: soft prompt 초기화 방법을 설정합니다. 기본값은 `RANDOM`입니다. \n",
        "    - `TEXT` 선택시 원하는 문장으로 초기화 할 수 있습니다. 추가적으로 2개 파라미터가 더 필요합니다.\n",
        "    - prompt_tuning_init_text: 초기화할 문장입니다.\n",
        "    - tokenizer_name_or_path: tokenizer 경로입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMAMA9diA9Z5"
      },
      "outputs": [],
      "source": [
        "# 토크나이저가 저장된 경로를 찾습니다.\n",
        "for path,dirs,files in os.walk('/root/.cache/huggingface/hub/models--kakaobrain--kogpt'):\n",
        "  for file in files:\n",
        "    if file.endswith('tokenizer.json'):\n",
        "      tokenizer_path = path\n",
        "print(tokenizer_path)\n",
        "\n",
        "peft_config = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    num_virtual_tokens=10,\n",
        "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
        "    \n",
        "    prompt_tuning_init_text=\"다음 글을 읽고 요약해줘:\",\n",
        "    tokenizer_name_or_path=tokenizer_path\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK0M7Awhlmzc"
      },
      "outputs": [],
      "source": [
        "print(peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(base_model, peft_config)\n",
        "peft_model.to('cuda')\n",
        "peft_model.train()"
      ],
      "metadata": {
        "id": "n2jnvfOXOi1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCSBeDEIn8Qy"
      },
      "source": [
        "## 학습\n",
        "\n",
        "- float32, float16이 섞여있으므로 amp autocast를 활용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMsfqYK_BQDx"
      },
      "outputs": [],
      "source": [
        "learning_rate = 3e-5\n",
        "\n",
        "optimizer = torch.optim.Adam(peft_model.parameters(), lr=learning_rate)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROqqOOIOGo5H"
      },
      "outputs": [],
      "source": [
        "def training_step(model, batch, optimizer, scaler):\n",
        "    optimizer.zero_grad()\n",
        "    with torch.cuda.amp.autocast():\n",
        "        outputs = model(\n",
        "            input_ids = batch['input_ids'],\n",
        "            attention_mask = batch['attention_mask'],\n",
        "            labels = batch['labels'],\n",
        "        )\n",
        "        step_loss = outputs[0]\n",
        "    scaler.scale(step_loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    return step_loss.detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11TzmgeoBQ0G"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 1\n",
        "\n",
        "peft_model.train()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_loss = 0\n",
        "    tr_loss = torch.tensor(0.0).to('cuda')\n",
        "    for batch_idx, batch in enumerate(tqdm(train_loader), start=1):\n",
        "        step_loss = training_step(peft_model, batch, optimizer, scaler)\n",
        "        tr_loss += step_loss\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\"{}. tr_loss: {}\".format(batch_idx, tr_loss.item()))\n",
        "            tr_loss = torch.tensor(0.0).to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7SpZpLCYPJK"
      },
      "outputs": [],
      "source": [
        "TIME_SERIAL = datetime.now(timezone(timedelta(hours=9))).strftime(\"%y%m%d-%H%M%S\")\n",
        "PEFT_MODEL_PATH = f'/content/drive/MyDrive/GPT_Competition/exp_{TIME_SERIAL}'\n",
        "peft_model.save_pretrained(PEFT_MODEL_PATH)\n",
        "print(PEFT_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObuX5yaY7k9-"
      },
      "source": [
        "## 추론\n",
        "- base 모델을 불러오고, 저장된 PEFT 모델을 불러옵니다.\n",
        "- 추론 과정은 참고자료2와 동일합니다. 추론 예제파일을 참고하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bnJiVYW7r6-"
      },
      "outputs": [],
      "source": [
        "# Install Library\n",
        "%pip install -q transformers datasets accelerate\n",
        "%pip install -q peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEMhzGBL4Xax"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
        "from peft import PeftModel\n",
        "import os, gc\n",
        "from datetime import datetime, timezone, timedelta\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxU7qOlg7mut"
      },
      "outputs": [],
      "source": [
        "# Google Drive Mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgPbYoWN4qPG"
      },
      "outputs": [],
      "source": [
        "# PEFT_MODEL_PATH = \"/content/drive/MyDrive/GPT_Competition/exp_230320-101642\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGYYcRnR4gsi"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmR8M5Ry5bwG"
      },
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    'kakaobrain/kogpt', revision = 'KoGPT6B-ryan1.5b-float16',\n",
        "    torch_dtype = torch.float16,\n",
        "    # device_map = 'auto',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BATwaoph4v1j"
      },
      "outputs": [],
      "source": [
        "model = PeftModel.from_pretrained(model=base_model, model_id=PEFT_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuP-alhLmpug"
      },
      "outputs": [],
      "source": [
        "model.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE4a6Io56P1g"
      },
      "outputs": [],
      "source": [
        "class SummaryTestDataset(Dataset):\n",
        "    def __init__(self, data_path, tokenizer):\n",
        "        self._data = pd.read_csv(data_path)\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self._data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self._data.iloc[idx]\n",
        "        prompt = \"{text} 한줄 요약:\"\n",
        "        input_text = prompt.format(text=row['text'])\n",
        "        input_encoding = self.tokenizer(input_text)\n",
        "\n",
        "        result = {\n",
        "            'input_ids': input_encoding['input_ids'],\n",
        "            'attention_mask': input_encoding['attention_mask'],\n",
        "        }\n",
        "        \n",
        "        return result\n",
        "\n",
        "    def _left_pad(self, sequence, value, max_len):\n",
        "        return [value] * (max_len - len(sequence)) + sequence\n",
        "\n",
        "    def collate_fn(self, batch, device='cuda'):\n",
        "        input_length = max(len(row['input_ids']) for row in batch)\n",
        "\n",
        "        input_ids = [\n",
        "            self._left_pad(row['input_ids'], self.tokenizer.pad_token_id, input_length)\n",
        "            for row in batch\n",
        "        ]\n",
        "        attention_mask = [\n",
        "            self._left_pad(row['attention_mask'], 0, input_length)\n",
        "            for row in batch\n",
        "        ]\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(input_ids, dtype=torch.long, device=device),\n",
        "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long, device=device),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mU03Jxao6wXb"
      },
      "outputs": [],
      "source": [
        "test_path = '/content/drive/MyDrive/GPT_Competition/test.csv'\n",
        "test_set = SummaryTestDataset(test_path, tokenizer)\n",
        "test_loader = DataLoader(test_set, batch_size=2, num_workers=0, shuffle=False, collate_fn=test_set.collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL4pw_yu6zyR"
      },
      "outputs": [],
      "source": [
        "def predict():\n",
        "    preds = []\n",
        "    model.eval()\n",
        "    for batch_idx, batch in enumerate(tqdm(test_loader)):\n",
        "        with torch.no_grad():\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                generated = model.generate(\n",
        "                    input_ids = batch['input_ids'],\n",
        "                    attention_mask = batch['attention_mask'],\n",
        "\n",
        "                    pad_token_id = tokenizer.pad_token_id,\n",
        "                    max_new_tokens = 100,\n",
        "                    do_sample = False,\n",
        "                    num_beams = 1,\n",
        "                    num_beam_groups = 1,\n",
        "                    penalty_alpha = None,\n",
        "                    use_cache = True,\n",
        "\n",
        "                    temperature = 1.0,\n",
        "\n",
        "                )\n",
        "            prompted_length = batch['input_ids'].size(-1)\n",
        "            summary_tokens = generated[:, prompted_length:]\n",
        "            summary = tokenizer.batch_decode(summary_tokens, skip_special_tokens=True)\n",
        "            preds.extend(summary)\n",
        "            print(*summary, sep='\\n----------\\n',end='\\n========\\n')\n",
        "    return preds\n",
        "\n",
        "preds = predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IaAyKWx65Z9"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(test_path)\n",
        "test_df['summary'] = preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5y2uCYvKYqq"
      },
      "outputs": [],
      "source": [
        "# 현재 시간으로 이름붙인 제출파일을 생성합니다.\n",
        "TIME_SERIAL = datetime.now(timezone(timedelta(hours=9))).strftime(\"%y%m%d-%H%M%S\")\n",
        "SUBMISSION_PATH = os.path.join(PEFT_MODEL_PATH, f\"{TIME_SERIAL}.csv\")\n",
        "test_df[['id', 'summary']].to_csv(SUBMISSION_PATH, index=False)\n",
        "print(SUBMISSION_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AShefe-trrsW"
      },
      "source": [
        "- 세션 종료하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDdyLHAfMQRD"
      },
      "outputs": [],
      "source": [
        "# 자동으로 세션을 종료하고 싶을때 사용하세요.\n",
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbEjgX7tqnDy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}